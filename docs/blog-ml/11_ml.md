# 第十一章：梯度下降法

## 一. 作用
1. 梯度下降法是一个基于搜索的最优化方法
2. 适用于没有数学特征的算法
3. 最小化一个损失函数
4. 相反，最大化一个效用函数应使用梯度上升法

## 二. 图解
![ml-11-1](https://s2.ax1x.com/2020/01/08/lgXD6P.md.png)

![ml-1-1](https://s2.ax1x.com/2020/01/15/lXPsbj.md.png)

![11-2](https://s2.ax1x.com/2020/01/15/lXp8Ds.md.png)

* 对于高维函数

![11-3](https://s2.ax1x.com/2020/01/15/lXpN5V.md.png)

* 有可能找到的解不是全局最优解
* 解决这个问题的方法是，多次运行

## 三. 线性回归使用梯度下降法
* 线性回归损失函数的梯度是一个向量

![11-4](https://s2.ax1x.com/2020/01/15/lXp0v4.md.png)

![11-5](https://s2.ax1x.com/2020/01/15/lXpLPf.md.png)



* 同样使用我们在线性回归中技巧，将yhead整理成两个向量相乘的模式

![11-6](https://s2.ax1x.com/2020/01/15/lX9uZR.md.png)

* 我们可以观察到m数量对梯度是有影响的，所以我们考虑到将目标函数除以m

![11-7](https://s2.ax1x.com/2020/01/15/lX93RO.md.png)


## 四. 向量化
![11-8](https://s2.ax1x.com/2020/01/15/lX90FP.md.png)

![11-9](https://s2.ax1x.com/2020/01/15/lXCSSO.md.png)

![11-10](https://s2.ax1x.com/2020/01/15/lXC17n.md.png)

* 我们得到结果



> 在梯度下降前，应对数据进行归一化

![11-11](https://s2.ax1x.com/2020/01/15/lXCUcF.md.png)

## 五. 随机梯度下降法

### 1. 批量梯度下降法的问题
* 计算量与m相关，但是我们又除以了m
* 有没有可能将sigma去掉，我们只取一个方向的损失函数的最小值

### 2. 图示

![11-12](https://s2.ax1x.com/2020/01/15/lXCrA1.md.png)

* 这个超参数的选择源自模拟退火的思想




<comment-comment/><comment/>